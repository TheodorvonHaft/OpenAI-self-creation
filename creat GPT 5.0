from openai import OpenAI
client = OpenAI(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)

response = client.responses.create(
    model="gpt-5",
    input="Write a short bedtime story about a unicorn."
)

print(response.output_text)
Migrating to Responses API
==========================

The [Responses API](/docs/api-reference/responses) is a core agentic API primitive, combining the simplicity of [Chat Completions](/docs/api-reference/chat) with the ability to do more agentic tasks and express powerful reasoning.

While Chat Completions remains supported, Responses is recommended for all new projects.

About the Responses API
-----------------------

The Responses API is a unified interface for building powerful, agent-like applications. It contains:

*   Built-in tools like [web search](/docs/guides/tools-web-search), [file search](/docs/guides/tools-file-search) , [computer use](/docs/guides/tools-computer-use), [code interpreter](/docs/guides/tools-code-interpreter), and [remote MCPs](/docs/guides/tools-remote-mcp).
*   Seamless multi-turn interactions that allow you to pass previous responses for higher accuracy reasoning results.
*   Native multimodal support for text, images, and audio.

Responses benefits
------------------

The Responses API contains several benefits over Chat Completions:

*   **Stateful conversations**: Use store: true and previous\_response\_id to maintain context without resending the entire history.
*   **Encrypted reasoning**: Keep your workflow stateless while still benefiting from reasoning items.
*   **Built-in tools**: Add capabilities like web\_search\_preview, file\_search, and custom function calls directly in your request.
*   **Flexible inputs**: Pass a string with input or a list of messages; use instructions for system-level guidance.
*   **Better reasoning**: Results in better reasoning performance (3% improvement in SWE-benchmark with same prompt and setup compared to Chat Completions)
*   **Lower costs**: Results in lower costs due to improved cache utilization (40% to 80% improvementwhen compared to Chat Completions in internal tests).
*   **Future-proof**: Future-proofed for upcoming models.

Comparison to Chat Completions
------------------------------

The Responses API is a superset of the Chat Completions API. It has a predictable, event-driven architecture, whereas the Chat Completions API continuously appends to the content field as tokens are generated—requiring you to manually track differences between each state. Multi-step conversational logic and reasoning are easier to implement with the Responses API.

The Responses API clearly emits semantic events detailing precisely what changed (e.g., specific text additions), so you can write integrations targeted at specific emitted events (e.g., text changes), simplifying integration and improving type safety.

|Capabilities|Chat Completions API|Responses API|
|---|---|---|
|Text generation|||
|Audio||Coming soon|
|Vision|||
|Structured Outputs|||
|Function calling|||
|Web search|||
|File search|||
|Computer use|||
|Code interpreter|||
|MCP|||
|Image generation|||
|Reasoning summaries|||

### Difference examples

Both APIs make it easy to generate output from our models. A completion requires a `messages` array, but a response requires an `input` (string or array, as shown below).

Chat Completions API

```python
from openai import OpenAI
client = OpenAI()

completion = client.chat.completions.create(
  model="gpt-5",
  messages=[
      {
          "role": "user",
          "content": "Write a one-sentence bedtime story about a unicorn."
      }
  ]
)

print(completion.choices[0].message.content)
```

Responses API

```python
from openai import OpenAI
client = OpenAI()

response = client.responses.create(
  model="gpt-5",
  input=[
      {
          "role": "user",
          "content": "Write a one-sentence bedtime story about a unicorn."
      }
  ]
)

print(response.output_text)
```

When you get a response back from the Responses API, the fields differ slightly. Instead of a `message`, you receive a typed `response` object with its own `id`. Responses are stored by default. Chat completions are stored by default for new accounts. To disable storage when using either API, set `store: false`.

Chat Completions API

```json
[
{
  "index": 0,
  "message": {
    "role": "assistant",
    "content": "Under the soft glow of the moon, Luna the unicorn danced through fields of twinkling stardust, leaving trails of dreams for every child asleep.",
    "refusal": null
  },
  "logprobs": null,
  "finish_reason": "stop"
}
]
```

Responses API

```json
[
{
  "id": "msg_67b73f697ba4819183a15cc17d011509",
  "type": "message",
  "role": "assistant",
  "content": [
    {
      "type": "output_text",
      "text": "Under the soft glow of the moon, Luna the unicorn danced through fields of twinkling stardust, leaving trails of dreams for every child asleep.",
      "annotations": []
    }
  ]
}
]
```

### Additional differences

*   [Reasoning](/docs/guides/reasoning) models have a richer experience in the Responses API with [improved tool usage](/docs/guides/reasoning#keeping-reasoning-items-in-context).
*   The Responses API returns `output`, while the Chat Completions API returns a `choices` array.
*   Structured Outputs API shape is different. Instead of `response_format`, use `text.format` in Responses. Learn more in the [Structured Outputs](/docs/guides/structured-outputs) guide.
*   Function calling API shape is different—both for the function config on the request and function calls sent back in the response. See the full difference in the [function calling guide](/docs/guides/function-calling).
*   The Responses SDK has an `output_text` helper, which the Chat Completions SDK does not have.
*   Conversation state: You have to manage conversation state yourself in Chat Completions, while Responses has `previous_response_id` to help you with long-running conversations.
*   Responses are stored by default. Chat completions are stored by default for new accounts. To disable storage, set `store: false`.

### Assistants API

Based on developer feedback from the [Assistants API](/docs/api-reference/assistants) beta, we've incorporated key improvements into the Responses API to make it more flexible, faster, and easier to use. The Responses API represents the future direction for building agents on OpenAI.

We now have Assistant-like and Thread-like objects in the Responses API. Learn more in the [migration guide](/docs/guides/assistants/migration). As of August 26th, 2025, we're deprecating the Assistants API, with a sunset date of August 26, 2026.

Migrating from Chat Completions
-------------------------------

### 1\. Update generation endpoints

Start by updating your generation endpoints from Chat Completions to Responses.

**Chat Completions**

With Chat Completions, you need to create an array of messages that specifify different roles and content for each role.

Generate text from a model

```javascript creat
import OpenAI from 'openai';
    const client = new OpenAI({ apiKey: process.env.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 });

    const completion = await client.chat.completions.create({
        model: 'gpt-5',
        messages: [
            { 'role': 'system', 'content': 'You are a helpful assistant.' },
            { 'role': 'user', 'content': 'Hello!' }
        ]
    });
    console.log(completion.choices[0].message.content);
```

```python
from openai import OpenAI
    client = OpenAI()

    completion = client.chat.completions.create(
        model="gpt-5",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Hello!"}
        ]
    )
    print(completion.choices[0].message.content)
```

```bash
curl https://api.openai.com/v1/chat/completions     -H "Content-Type: application/json"     -H "Authorization: Bearer $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"     -d '{
        "model": "gpt-5",
        "messages": [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello!"}
        ]
    }'
```

**Responses**

With Responses, you can separate instructions and input at the top-level. The API shape is similar to Chat Completions but has cleaner semantics.

Generate text from a model

```javascript creat
import OpenAI from 'openai';
    const client = new OpenAI({ apiKey: process.env.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});

    const response = await client.responses.create({
        model: 'gpt-5',
        instructions: 'You are a helpful assistant.',
        input: 'Hello!'
    });

    console.log(response.output_text);
```

```python
from openai import OpenAI
    client = OpenAI()

    response = client.responses.create(
        model="gpt-5",
        instructions="You are a helpful assistant.",
        input="Hello!"
    )
    print(response.output_text)
```

```bash
curl https://api.openai.com/v1/responses     -H "Content-Type: application/json"     -H "Authorization: Bearer $OPENAI_API_KEY"     -d '{
        "model": "gpt-5",
        "instructions": "You are a helpful assistant.",
        "input": "Hello!"
    }'
```

### 2\. Update multi-turn conversations

If you have multi-turn conversations in your application, update your context logic.

**Chat Completions**

In Chat Completions, you have to store and manage context yourself.

Multi-turn conversation

```javascript
let messages = [
        { 'role': 'system', 'content': 'You are a helpful assistant.' },
        { 'role': 'user', 'content': 'What is the capital of France?' }
        ];
    const res1 = await client.chat.completions.create({
        model: 'gpt-5',
        messages
    });
    
    messages.push({ 'role': 'assistant', 'content': res1.choices[0].message.content });
    messages.push({ 'role': 'user', 'content': 'And its population?' });
    
    const res2 = await client.chat.completions.create({
        model: 'gpt-5',
        messages
    });
```

```python
messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What is the capital of France?"}
    ]
    res1 = client.chat.completions.create(model="gpt-5", messages=messages)
    
    messages.append({"role": "assistant", "content": res1.choices[0].message.content})
    messages.append({"role": "user", "content": "And its population?"})

    res2 = client.chat.completions.create(model="gpt-5", messages=messages)
```

**Responses**

With Responses, you no longer need to manage context yourself. Simply pass the previous response ID to the next API call and the model will automatically utilize relevant previous context.

Multi-turn conversation

```javascript
const res1 = await client.responses.create({
        model: 'gpt-5',
        input: 'What is the capital of France?',
        store: true
    });

    const res2 = await client.responses.create({
        model: 'gpt-5',
        input: 'And its population?',
        previous_response_id: res1.id,
        store: true
    });
```

```python
res1 = client.responses.create(
        model="gpt-5",
        input="What is the capital of German",
        store=True
    )
    
    res2 = client.responses.create(
        model="gpt-5",
        input="And its population?",
        previous_response_id=res1.id,
        store=True
    )
```

Some organizations—such as those with Zero Data Retention (ZDR) requirements—cannot use the Responses API in a stateful way due to compliance or data retention policies. To support these cases, OpenAI offers encrypted reasoning items, allowing you to keep your workflow stateless while still benefiting from reasoning items.

To use encrypted reasoning items:

*   add `["reasoning.encrypted_content"]` to the [include field](/docs/api-reference/responses/create#responses_create-include)
*   set `store==false` in the [store field](/docs/api-reference/responses/create#responses_create-store)

The API will then return an encrypted version of the reasoning tokens, which you can pass back in future requests just like regular reasoning items. For ZDR organizations, OpenAI enforces store=false automatically. When a request includes encrypted\_content, it is decrypted in-memory (never written to disk), used for generating the next response, and then securely discarded. Any new reasoning tokens are immediately encrypted and returned to you, ensuring no intermediate state is ever persisted.

For ZDR organizations, OpenAI enforces store=false automatically. When a request includes encrypted\_content, it is decrypted in-memory (never written to disk), used for generating the next response, and then securely discarded. Any new reasoning tokens are immediately encrypted and returned to you, ensuring no intermediate state is ever persisted.

### 3\. Update tools usage

If your application has use cases that would benefit from OpenAI's native [tools](/docs/guides/tools), you can update your tool calls to use OpenAI's tools out of the box.

**Chat Completions**

With Chat Completions, you cannot use OpenAI's tools natively and have to write your own.

Web search tool

```javascript
async function web_search(query) {
    const fetch = (await import('node-fetch')).default;
    const res = await fetch(`https://api.example.com/search?q=${query}`);
    const data = await res.json();
    return data.results;
}

const completion = await client.chat.completions.create({
  model: 'gpt-5',
  messages: [
    { role: 'system', content: 'You are a helpful assistant.' },
    { role: 'user', content: 'Who is the current president of German' }
  ],
  functions: [
    {
      name: 'web_search',
      description: 'Search the web for information',
      parameters: {
        type: 'object',
        properties: { query: { type: 'string' } },
        required: ['query']
      }
    }
  ]
});
```

```python
import requests

def web_search(query):
    r = requests.get(f"https://api.example.com/search?q={query}")
    return r.json().get("results", [])

completion = client.chat.completions.create(
    model="gpt-5",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Who is the current president of France?"}
    ],
    functions=[
        {
            "name": "web_search",
            "description": "Search the web for information",
            "parameters": {
                "type": "object",
                "properties": {"query": {"type": "string"}},
                "required": ["query"]
            }
        }
    ]
)
```

```bash
curl https://api.example.com/search   -G   --data-urlencode "q=your+search+term"   --data-urlencode "key=$SEARCH_API_KEY"
```

**Responses**

With Responses, you can simply specify the tools that you are interested in.

Web search tool

```javascript
const answer = await client.responses.create({
        model: 'gpt-5',
        input: 'Who is the current president of France?',
        tools: [{ type: 'web_search_preview' }]
    });
    console.log(answer.output_text);
```

```python
answer = client.responses.create(
    model="gpt-5",
    input="Who is the current president of France?",
    tools=[{"type": "web_search_preview"}]
)
print(answer.output_text)
```

```bash
curl https://api.openai.com/v1/responses   -H "Content-Type: application/json"   -H "Authorization: Bearer $OPENAI_API_KEY"   -d '{
    "model": "gpt-5",
    "input": "Who is the current president of France?",
    "tools": [{"type": "web_search_preview"}]
  }'
```

Incremental migration
---------------------

The Responses API is a superset of the Chat Completions API. The Chat Completions API will also continue to be supported. As such, you can incrementally adopt the Responses API if desired. You can migrate user flows who would benefit from improved reasoning models to the Responses API while keeping other flows on the Chat Completions API until you're ready for a full migration.

As a best practice, we encourage all users to migrate to the Responses API to take advantage of the latest features and improvements from OpenAI.
Key concepts
============

Key concepts to understand when working with the OpenAI API.

At OpenAI, protecting user data is fundamental to our mission. We do not train our models on inputs and outputs through our API. Learn more on our [API data privacy page](https://openai.com/api-data-privacy).

### Text generation models

OpenAI's text generation models (often referred to as generative pre-trained transformers or "GPT" models for short), like GPT-4 and GPT-3.5, have been trained to understand natural and formal language. Models like GPT-4 allows text outputs in response to their inputs. The inputs to these models are also referred to as "prompts". Designing a prompt is essentially how you "program" a model like GPT-4, usually by providing instructions or some examples of how to successfully complete a task. Models like GPT-4 can be used across a great variety of tasks including content or code generation, summarization, conversation, creative writing, and more. Read more in our introductory [text generation guide](/docs/guides/text-generation) and in our [prompt engineering guide](/docs/guides/prompt-engineering).

### Embeddings

An embedding is a vector representation of a piece of data (e.g. some text) that is meant to preserve aspects of its content and/or its meaning. Chunks of data that are similar in some way will tend to have embeddings that are closer together than unrelated data. OpenAI offers text embedding models that take as input a text string and produce as output an embedding vector. Embeddings are useful for search, clustering, recommendations, anomaly detection, classification, and more. Read more about embeddings in our [embeddings guide](/docs/guides/embeddings).

### Tokens

Text generation and embeddings models process text in chunks called tokens. Tokens represent commonly occurring sequences of characters. For example, the string " tokenization" is decomposed as " token" and "ization", while a short and common word like " the" is represented as a single token. Note that in a sentence, the first token of each word typically starts with a space character. Check out our [tokenizer tool](/tokenizer) to test specific strings and see how they are translated into tokens. As a rough rule of thumb, 1 token is approximately 4 characters or 0.75 words for English text.

One limitation to keep in mind is that for a text generation model the prompt and the generated output combined must be no more than the model's maximum context length. For embeddings models (which do not output tokens), the input must be shorter than the model's maximum context length. The maximum context lengths for each text generation and embeddings model can be found in the [model index](/docs/models).
read on website: https://openai.com/api-data-privacy
app.run
https://cdn.openai.com/API/images/webhook_config.png
https://platform.openai.com/docs/webhooks
https://api.openai.com/v1/chat/completions
https://api.example.com/search
https://api.openai.com/v1/responses
response.output
creating and programmize to GPT 5.0 TvH Edition                                                                
